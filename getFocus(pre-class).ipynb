{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "First of all, I will create a working notebook for getting the required data, and ONLY THEN transform it into a class. Amateurish mistake, wasted 5 hours on this already.\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "import gensim\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    '''\n",
    "    Removes any url from a string.\n",
    "    \n",
    "    -----\n",
    "    Returns: Same string format, with url removed.\n",
    "    \n",
    "    -----\n",
    "    Arguments\n",
    "    \n",
    "    text (str): string to have url removed from it.\n",
    "    '''\n",
    "    sent = text.split()\n",
    "    sent = [re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', word, flags=re.MULTILINE) for word in sent]  \n",
    "    return(' '.join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing variables.\n",
    "n_clusters = 3\n",
    "file_format = 'json'\n",
    "\n",
    " # loading neural coref's pre-trained dataset. Source: https://github.com/aukslius12/neuralcoref \n",
    "nlp = spacy.load('en_coref_lg')\n",
    "\n",
    "# ----------------------------- TEMPORARY ---------\n",
    "# Loading twitter data. NOTE: Will be changed to a text loading method!!\n",
    "if file_format == 'json':\n",
    "    tweets = pd.read_json('twitter_data.json')['text'].values\n",
    "elif file_format == 'csv':\n",
    "    tweets = pd.read_csv('twitter_data.csv')['text'].values\n",
    "    \n",
    "# Creating a tweet dataframe\n",
    "tweets_raw = []\n",
    "for tweet in tweets:\n",
    "    tweet_no_url = remove_url(tweet)\n",
    "    tweets_raw.append(tweet_no_url)\n",
    "\n",
    "# Output (so far) should be a list of strings.\n",
    "# ---------------------------------------------------\n",
    "text_list = tweets_raw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_clust(word):\n",
    "        '''\n",
    "        Iterates through entity names (clusters) to see if the word is entity (cluster).\n",
    "        \n",
    "        -----\n",
    "        Attributes\n",
    "        \n",
    "        word (str): word to check for being an entity (cluster).\n",
    "        '''\n",
    "        if word in clust_names:\n",
    "            return 1\n",
    "        else: \n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_focus(text):\n",
    "text = ''\n",
    "# Gets cluster.\n",
    "result = get_clust(text)\n",
    "\n",
    "if result == None:\n",
    "    # Check for focus in the original text.\n",
    "else:\n",
    "    # Split\n",
    "    # Removed if no focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clust(text):\n",
    "    \n",
    "    # Parsing with neural coref's pre-trained dataset.\n",
    "    doc = nlp(text)\n",
    "    n_nearest = 4\n",
    "    \n",
    "    # Checks if any coreferences are present.\n",
    "    if doc._.has_coref:\n",
    "        # Replaces coreferences with the reference itself. \n",
    "        # Example: \n",
    "        # \"Lithuania is ok. Its people are great.\" -> \"Lithuania is ok. Lithuania people are great.\"\n",
    "        corefed_doc = doc._.coref_resolved \n",
    "        \n",
    "        # Gets names of the entities (clusters).\n",
    "        clust_names = []\n",
    "        for clust in doc._.coref_clusters:\n",
    "            clust_names.append(str(clust[0]).lower())\n",
    "\n",
    "        # Preprocess.\n",
    "        text = gensim.utils.simple_preprocess(corefed_doc)\n",
    "\n",
    "        del doc\n",
    "        del corefed_doc\n",
    "        del clust\n",
    "\n",
    "        # Initialize the final dataframe and word cluster list (stores temporary cluster counters).\n",
    "        clust_data = pd.DataFrame({\n",
    "            'word': text\n",
    "        })\n",
    "        clust_data['is_cluster'] = clust_data['word'].map(lambda x: 1 if x in clust_names else 0)\n",
    "        clust_final = []\n",
    "        \n",
    "        # Run through every word in the text.\n",
    "        for ind in range(len(clust_data['word'])):\n",
    "\n",
    "            # Get the position of the word in the text.\n",
    "            ind_up, ind_dn = ind, ind\n",
    "\n",
    "            # Initializing counters for matches.\n",
    "            match_count = 0\n",
    "            clust_match_count = np.repeat(0, len(clust_names))\n",
    "\n",
    "            # Iterating \"upwards\".\n",
    "            while ind_up >= 0:\n",
    "                # Iterating until a cluster is present. \n",
    "                if clust_data.loc[ind_up]['is_cluster']:\n",
    "                    # Get the int \"name\" of the cluster.\n",
    "                    clust_num = clust_names.index(clust_data.loc[ind_up]['word'])\n",
    "\n",
    "                    # Increasing the counters.\n",
    "                    clust_match_count[clust_num] += 1\n",
    "                    match_count += 1\n",
    "\n",
    "                    if match_count >= n_nearest:\n",
    "                        break\n",
    "                ind_up -= 1\n",
    "\n",
    "            # Iterating downwards.\n",
    "            while ind_dn < clust_data.shape[0]:\n",
    "                # Iterating until a cluster is present.\n",
    "                if clust_data.loc[ind_dn]['is_cluster']:\n",
    "                    # Get the int \"name\" of the cluster\n",
    "                    clust_num = clust_names.index(clust_data.loc[ind_dn]['word'])\n",
    "\n",
    "                    # Increasing the counters.\n",
    "                    clust_match_count[clust_num] += 1\n",
    "                    match_count += 1\n",
    "\n",
    "                    if match_count >= n_nearest:\n",
    "                        break\n",
    "                ind_dn += 1\n",
    "\n",
    "            clust_final.append(clust_match_count.argmax())\n",
    "        \n",
    "        return clust_final\n",
    "        \n",
    "    else:\n",
    "        return pd.DataFrame() # If no coreferences are present in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_clust('Two Irish held in anti-terrorist operation: Lithuania said today that it has detained two Irish citizens. http://tinyurl.com/255loq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ------------------------ ##\n",
    "# Case studies below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Focus = Lithuania, no coreferences.\n",
    "doc = nlp('back from Lithuania. what a great place')\n",
    "doc._.has_coref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lithuania: [Lithuania, it]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Focus = Lithuania, coreferences indicate only lithuania.\n",
    "doc = nlp('Two Irish held in anti-terrorist operation: Lithuania said today that it has detained two Irish citizens. http://tinyurl.com/255loq')\n",
    "doc._.coref_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lithuania: [Lithuania, Its, its], Belarus: [Belarus, Belarus']]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Focus = Lithuania, coreferences indicate multiple entities.\n",
    "doc = nlp(\"Lithuania is a country and the southernmost of Europeâ€™s Baltic states, a former Soviet bloc nation bordering Poland, Latvia and Belarus. Its capital, Vilnius, near Belarus' border, is known for its medieval Old Town.\")\n",
    "doc._.coref_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Latvia: [Latvia, they, Latvia], Lithuania: [Lithuania, Lithuania]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Focus = else, coreferences indicate multiple entities (including Lithuania).\n",
    "doc = nlp(\"Latvia? Lithuania? Do they still embrace Communism? Latvia is way better than Lithuania\")\n",
    "doc._.coref_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Focus = else, no coreferences.\n",
    "doc = nlp(\"Ukraine is under attack. Georgia has been absorbed against the wishes of their citizenry. Lithuania is reporting... http://fb.me/6H6hKVJjnÃ‚\")\n",
    "doc._.has_coref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def split_by_focus(text, clustered_df):\\n    focus = clustered_df.iloc[0,1]\\n    print (focus)\\n    for word_index in range(clustered_df.shape[0]):\\n        print()\\n        \\n        \\nsplit_by_focus(clustered_df)'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SCRAPED: \n",
    "# Idea: rolling focus window: each word has an extended window of focus. For example, focus of \"great\" would be _dog / dog_ _dog / cat_ _cat / cat_, based on nearby words' focus window. Split where _focus1 / focus2_, keep where _focus1 / focus1_ .\n",
    "# Might have problems due to losing semantic information: \"dog was great.\";\"cat\" or \"dog was.\";\"great cat\".\n",
    "# Completely different meanings.\n",
    "\n",
    "'''\n",
    "clustered_df = pd.DataFrame({\n",
    "    'word': ['I', 'had', 'a', 'dog', 'dog', 'was', 'great', 'cat', 'was', 'a', 'bad', 'cat', 'I', 'love', 'my', 'dog', 'he', 'is', 'the', 'best'],\n",
    "    'belongs_to_cluster': ['dog', 'dog', 'dog', 'dog', 'dog', 'dog', 'dog', 'cat', 'cat', 'cat', 'cat', 'cat', 'dog', 'dog', 'dog', 'dog', 'dog', 'dog', 'dog', 'dog']\n",
    "})\n",
    "text = \"I had a dog. Dog was great, but cat was a bat cat. I love my dog - he is the best!\"\n",
    "'''\n",
    "\n",
    "## Example goal:\n",
    "# INPUT:  (text): I had a dog. Dog was great, but cat was a bat cat. I love my dog - he is the best!;\n",
    "#         (clustered_df): clustered_df\n",
    "# OUTPUT: (sentence): I had a dog. Dog was great (focus): dog\n",
    "#         (sentence): , but cat was a bat cat. (focus): cat\n",
    "#         (sentence): I love my dog - he is the best! (focus): dog\n",
    "#\n",
    "'''def split_by_focus(text, clustered_df):\n",
    "    focus = clustered_df.iloc[0,1]\n",
    "    print (focus)\n",
    "    for word_index in range(clustered_df.shape[0]):\n",
    "        print()\n",
    "        \n",
    "        \n",
    "split_by_focus(clustered_df)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_df = pd.DataFrame({\n",
    "    'word': ['I', 'had', 'a', 'dog', 'dog', 'was', 'great', 'cat', 'was', 'a', 'bad', 'cat', 'I', 'love', 'my', 'dog', 'he', 'is', 'the', 'best'],\n",
    "    'belongs_to_cluster': ['dog', 'dog', 'dog', 'dog', 'dog', 'dog', 'dog', 'cat', 'cat', 'cat', 'cat', 'cat', 'dog', 'dog', 'dog', 'dog', 'dog', 'dog', 'dog', 'dog']\n",
    "})\n",
    "focus_term = \"dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asad'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_priority_sentences(text, focus_term):\n",
    "    '''\n",
    "    returns a list of strings, that match with the focus_term.\n",
    "    '''\n",
    "    #clustered_df = get_clusters(text) \n",
    "    if clustered_df.empty:\n",
    "        print('')\n",
    "        # TODO: Method for identifying if focus = focus_term from base text.\n",
    "        # Use pre-trained word2vec \"similarities\" to identify synonyms of the search term.\n",
    "        # Replace synonyms with the focus_term.\n",
    "        # Count named entity / noun frequency. \n",
    "        # If most common = focus_term, good.\n",
    "        \n",
    "        # If named entity is only one and named entity = focus_term, focus = focus_term\n",
    "        # If focus_term mention frequency > 10% (arbitrary rule), focus = focus_term\n",
    "    else:\n",
    "        # Gets most commonly occuring cluster.\n",
    "        most_common_clusters = clustered_df['belongs_to_cluster'].mode().values\n",
    "        \n",
    "        # If focus term is amongst the common clusters. There can be multiple common clusters.\n",
    "        if (focus_term in most_common_clusters):\n",
    "            return (text) # Green-lights the original text.\n",
    "        else:\n",
    "            return '' # Returns an empty string.\n",
    "\n",
    "get_priority_sentences('asad', focus_term)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
