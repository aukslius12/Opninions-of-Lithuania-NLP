{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3-5.2.0\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "E:\\Anaconda3-5.2.0\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n",
      "E:\\Anaconda3-5.2.0\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "E:\\Anaconda3-5.2.0\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n",
      "E:\\Anaconda3-5.2.0\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "import gensim\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class getFocus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    '''\n",
    "    Removes any url from a string.\n",
    "    \n",
    "    -----\n",
    "    Returns: Same string format, with url removed.\n",
    "    \n",
    "    -----\n",
    "    Arguments\n",
    "    \n",
    "    text (str): string to have url removed from it.\n",
    "    '''\n",
    "    sent = text.split()\n",
    "    sent = [re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', word, flags=re.MULTILINE) for word in sent]  \n",
    "    return(' '.join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(n_clusters = 3, file_name = 'twitter_data.json', file_format = 'json'):\n",
    "    '''\n",
    "    TODO:\n",
    "    \n",
    "    -----\n",
    "    Arguments\n",
    "    \n",
    "    n_clusters (int): TODO:\n",
    "    \n",
    "    file_name (str): TODO:\n",
    "    \n",
    "    file_format (str): TODO:\n",
    "    '''\n",
    "    # Initializing class-level variables.\n",
    "    self.n_clusters = n_clusters \n",
    "    \n",
    "     # loading neural coref's pre-trained dataset. Source: https://github.com/aukslius12/neuralcoref \n",
    "    self.nlp = spacy.load('en_coref_lg')\n",
    "    \n",
    "    # ----------------------------- TEMPORARY ---------\n",
    "    # Loading twitter data. NOTE: Will be changed to a text loading method!!\n",
    "    if file_format == 'json':\n",
    "        tweets = pd.read_json('twitter_data.json')['text'].values\n",
    "    elif file_format == 'csv':\n",
    "        tweets = pd.read_csv('twitter_data.csv')['text'].values\n",
    "        \n",
    "    # Creating a tweet dataframe\n",
    "    self.tweets_raw = []\n",
    "    for tweet in tweets:\n",
    "        tweet_no_url = self.remove_url(tweet)\n",
    "        self.tweets_raw.append(tweet_no_url)\n",
    "    \n",
    "    # Output (so far) should be a list of strings.\n",
    "    # ---------------------------------------------------\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(self, tweet):\n",
    "    '''\n",
    "    TODO:\n",
    "    \n",
    "    -----\n",
    "    Attributes\n",
    "    \n",
    "    tweet (str): TODO:\n",
    "    '''\n",
    "    \n",
    "    def is_clust(word):\n",
    "        '''\n",
    "        Iterates through entity names (clusters) to see if the word is entity (cluster).\n",
    "        \n",
    "        -----\n",
    "        Attributes\n",
    "        \n",
    "        word (str): word to check for being an entity (cluster).\n",
    "        '''\n",
    "        if word in cluster_names:\n",
    "            return 1\n",
    "        else: \n",
    "            return 0\n",
    "    \n",
    "    def iterate(direction):\n",
    "        '''\n",
    "        Iterates sthrough\n",
    "        '''\n",
    "        while ind_up >= 0:\n",
    "            # Iterating until a cluster is present. \n",
    "            if clust_data.loc[ind_up]['is_cluster']:\n",
    "                # Get the int \"name\" of the cluster.\n",
    "                clust_num = clust_names.index(clust_data.loc[ind_up]['word'])\n",
    "                \n",
    "                # Increasing the counters.\n",
    "                clust_match_count[clust_num] += 1\n",
    "                match_count += 1\n",
    "                \n",
    "                if match_count >= self.n_nearest:\n",
    "                    break\n",
    "            ind_up -= 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
